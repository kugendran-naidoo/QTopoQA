Thoroughly analyze "k_mac_train_topoqa.py" with all its dependent libraries and "mac_run_training.zsh" which calls the python code, including all the input data, parameters and model output that it generates in the "experiments" directory.

Thoroughly analyze "k_mac_inference_pca_tsne4.py" with all its dependent libraries and "b0_mac_run_inference_batch_BM55-AF2.zsh" which calls the python code, including all the input data, parameters and output that it generates in the "logs" directory. "k_mac_inference_pca_tsne4.py" calls the model "topoqa.ckpt" which is the best model that has been trained.
I have used "k_mac_train_topoqa.py" and trained on presumably the exact same input and parameters, except I'm using ARM CPU only with no GPU support, and generated model "topo_best_val_loss=0.14407.ckpt.DETERM". When I run inference using this model instead of "topoqa.ckpt" I dont get the same results as from ""topoqa.ckpt".

Thoroughly analyze "topoqa.ckpt" and "topo_best_val_loss=0.14407.ckpt.DETERM" to determine why "k_mac_train_topoqa.py" with presumably all the same training input etc., and only using ARM CPU with no GPU is not generating a model that is exactly or close enough to "topoqa.ckpt". What are the possible reasons for this ? Can using a difference CPU and no GPU cause this?


How Training Works (k_mac_train_topoqa.py + mac_run_training.zsh)

mac_run_training.zsh seeds Python (PYTHONHASHSEED=222), Lightning workers (PL_SEED_WORKERS=1), and forces deterministic Torch kernels (TORCH_USE_DETERMINISTIC_ALGORITHMS=1). It then launches
python k_mac_train_topoqa.py --graph_dir graph_data ... --save_dir ./experiments.
k_mac_train_topoqa.py reads the pre-generated graphs in topoqa_train/graph_data/*.pt. For each file it clears any stale .batch, attaches the DockQ label from train.csv/val.csv, and yields a Batch object (wrapped in a single-element list) to satisfy the model’s expected input shape.
The model being trained is topoqa_train/gat_5_edge1.py::GNN_edge1_edgepooling. It contains three edge-aware GATv2 layers (dropout 0.25), edge pooling, and a sigmoid regression head. Lightning handles training and validation and writes checkpoints to ./experiments/ (topo_last.ckpt, topo_best_val_loss=*.ckpt).

How Inference Works (k_mac_inference_pca_tsne4.py + b0_mac_run_inference_batch_BM55-AF2.zsh)

b0_mac_run_inference_batch_BM55-AF2.zsh orchestrates the inference run: for each target, it builds graphs on the fly from PDBs using topoqa/src/get_interface.py, src/topo_feature.py, src/node_fea_df.py, and src/graph.py.
k_mac_inference_pca_tsne4.py instantiates topoqa/src/proteingat.py::GNN_edge1_edgepooling (an inference-only copy of the same architecture), loads model/topoqa.ckpt, and computes DockQ predictions, writing results into the appropriate subfolders under scripts/inference/topoqa/mac/logs/.

Architecture Alignment

The Lightning model (topoqa_train/gat_5_edge1.py) and the inference model (topoqa/src/proteingat.py) define the same module names (embed, edge_embed, conv1/2/3, protein_fc_1, fc_edge, fc1, out). That is why checkpoint['state_dict'] from training can be loaded into the inference class without renaming keys.
Inference runs inside model.eval() with torch.no_grad(), so the dropout used during training is disabled.

Why Your topo_best_val_loss=0.14407.ckpt.DETERM Behaves Differently

Even with identical CLI arguments, several factors can prevent bit-for-bit reproduction of topoqa.ckpt.original:

Environment Differences

The published checkpoint was trained on x86_64 (likely with CUDA). Your run on Apple/ARM uses a different BLAS (Accelerate vs MKL) and a different PyTorch build. Many floating-point operations return slightly different results across architectures, and those differences accumulate over 200 epochs.

Library Versions

The shipped data (graph_data/*.pt) embeds features generated by Gudhi, Biopython, and scikit-learn. If you regenerated those graphs on ARM, Gudhi’s Vietoris–Rips and Alpha complexes may order simplices differently, leading to different persistent-homology statistics. The repo contains a cached set; use those untouched if you aim for strict reproducibility.

PyTorch Lightning has evolved; if topoqa.ckpt.original was trained with an older Lightning/PyTorch stack, subtle behavioural changes (e.g., how gradients are reduced) can occur even with matched seeds.

Determinism Extras

k_mac_train_topoqa.py sets torch.use_deterministic_algorithms(True) and seeds CPU RNGs, but it doesn’t set torch.backends.cudnn.deterministic (irrelevant on ARM) or guard against multi-worker data loader variation. The data loader uses num_workers=9 and persistent_workers=True, so each worker maintains its own RNG stream; small differences in scheduling can change dropout masks or batch accumulation order and lead to different minima.
Ensure PL_SEED_WORKERS=1 (already set in mac_run_training.zsh), but if you want stricter reproducibility, reduce num_workers to 0 (sequential loading) as recommended in the deterministic GPU guide.

Checkpoint Origin

topoqa.ckpt.original is a Lightning checkpoint; it contains state_dict, optimizer state, and Lightning metadata. The inference script expects the state_dict portion only. Your newly trained checkpoint is also Lightning-formatted, so swapping files should work. If you rename it to model/topoqa.ckpt but inference still diverges, it indicates the learned weights genuinely differ, not a loading mismatch.

Hardware Limitations

Training solely on CPU changes the optimisation trajectory. With Adam and gradient accumulation (32 × 16 = effective batch 512), GPU and CPU runs can land in different local minima even with identical seeds. GPU/CPU also use different low-level kernels (cuBLAS vs BLAS/LAPACK) whose numerical noise shifts the gradient path.

Recommended Steps to Get Closer

Use the cached graph_data shipped with the project rather than regenerating on ARM.

Run the official training recipe on the same architecture used originally: an x86_64 machine with CUDA. The repository already includes an AWS-ready GPU guide (qtdaqa/proposed_model_training/INTEL+GPU) that pins versions and environment variables for reproducible runs on a g5.xlarge/g5.2xlarge.

If you must train on macOS ARM, minimise nondeterministic sources:
Apply the provided patch to set num_workers=0.
Disable persistent workers.
Keep torch.use_deterministic_algorithms(True) and confirm no warnings.
Ensure no other code mutates the RNG state (e.g., WandB, TensorBoard).
After training, confirm you are pointing inference at the new checkpoint (topoqa/model/topoqa.ckpt or pass --checkpoint flag if you add one).
Compare predictions on a single target (e.g., run inference with both checkpoints and diff the CSV output). If errors are small (≈1e‑3), that’s expected numerical drift; large deviations indicate a dataset or training mismatch.
In short, the mismatch isn’t due to an architectural bug—the inference code matches the training model. The main culprits are platform differences (ARM vs x86), nondeterministic data loading, and potential library version drift. Training on the same x86 NVIDIA setup—or adopting the GPU reproduction instructions already in the repo—should bring your checkpoint in line with topoqa.ckpt.original.
