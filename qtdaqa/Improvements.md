Improvements

- modify calculate_interface.py
    - remove redundant PDB parser
    - remove all hard-coded values
    - add KDTree implementation to improve parsing PDB file
        - https://chatgpt.com/g/g-p-68c3bcec209081919551218d8aba08b5-shell-python/c/68de63af-7730-8326-9acb-e43c45eeac51

- modify topological_features.py
    - remove all hard-coded values


Issues

Pipeline Weaknesses

Interface extraction only attempts model.pdb files (topoqa/src/get_interface.py:61-65); decoys distributed as .cif/.mmcif will never enter the pipeline unless duplicate .pdb copies exist, leading to missing graphs and lower recall.

Node features depend on mkdssp being discoverable (topoqa/src/node_fea_df.py:43-73); if the binary is absent the script raises, yet the shell wrappers have no pre-flight check for it, so production runs can fail late.

utils.get_pointcloud_type writes the z-coordinate as coord[1] (topoqa/src/utils.py:85-99), flattening 3D distances into a plane and corrupting all histogram-based edge attributes; this likely inflates inference variance and should be corrected before retraining.

Graph construction again hard-codes .pdb input (topoqa/src/graph.py:12-28) and never attaches labels, so the .pt files generated by inference lack targets; training scripts must rewrite .y from CSVs every time.

The per-target embedding writer assumes decoy IDs differ only by simple suffixes and normalises with regexes (topoqa/k_mac_inference_pca_tsne4.py:175-276); collisions between cleaned IDs can silently mix labels.

Training Pipeline

topoqa_train/gpu_run_training.zsh:1-12 always points at topoqa_train/graph_data, so the ARM-generated .pt files are ignored unless you swap directories manually; it also unconditionally requests a CUDA device.

topoqa_train/train_topoqa.py:77-99 returns [graph_data] from the dataset, but the collate function expects Data objects and Lightning later indexes train_batch[0], so the first tensor column is treated as a sample; combined with the missing .y assignment (train_topoqa.py:84-87), the GPU path cannot learn meaningful weights.

Training and validation loaders never shuffle (train_topoqa.py:98-115), so even if the script were fixed the optimiser would see data in a fixed order with an effective batch of 16×32=512, giving only ~17 optimiser steps per epoch.

The working training entry point is topoqa_train/k_mac_train_topoqa.py, which rebuilds labels, sanitises stale batch attributes, and runs on CPU (topoqa_train/k_mac_train_topoqa.py:134-248); it also copies the best checkpoint for reproducibility.

gat_5_edge1.py:205-282 logs only MSE yet keeps per-target Pearson/Spearman metrics; without explicit early stopping or LR scheduling, the high accumulate setting can still plateau, so monitoring val_loss is essential.

Mini Tests

Coverage against the ARM .pt directory is high but not complete: 8684/8723 training labels and 3366/3397 validation labels had matching graphs (Python check run in-session).

Using manual batching (Batch.from_data_list) with the published topoqa/model/topoqa.ckpt, I measured train MSE 0.0133 and val MSE 0.0557 on the ARM graphs; MAEs were 0.0747 and 0.1483 respectively. This is close but ~0.006 above the reported 0.0498 target, suggesting coverage gaps or subtle feature drift.

39 training and 31 validation decoys lacked graphs; they skew the dockq distribution slightly towards lower-quality models in the ARM set, so supplementing or regenerating those graphs should help close the remaining MSE gap.

CPU vs GPU

The maintained trainer already targets CPU (topoqa_train/k_mac_train_topoqa.py:237-243) and delivers a Lightning best_model_score of 0.0498, so matching the reference MSE is feasible without a GPU provided you accept longer epochs (~5-6× slower in earlier runs).

GPU acceleration benefits come mainly from joblib feature extraction and Lightning optimisation time; inference itself is CPU-bound until graph tensors are loaded, so a GPU mostly shortens training iterations rather than improving fit quality.

If you remain on CPU, keep accumulate_grad_batches modest and prefer deterministic seeds (k_mac_train_topoqa.py:126-154) to stabilise convergence; the metric delta we observed appears attributable to missing graphs rather than device choice.

Next Steps

Repair feature geometry (utils.py:85-99) and regenerate graphs so all CSV labels have matching .pt artefacts before retraining.

Either patch train_topoqa.py to set graph_data.y, return plain Data objects, and use shuffling, or retire it in favour of k_mac_train_topoqa.py; then rerun training on the updated graph set and re-evaluate MSE on the ARM data.

Graph Builder 2 Overview

qtdaqa/graph_builder2/graph_builder2.py reproduces the full inference data flow locally: it validates dataset/work/log paths, extracts interfaces for every .pdb/.cif via new_calculate_interface.process_pdb_file, rounds coordinates, and records per-model logs (graph_builder2.py:260-418).

Topological descriptors are recomputed with the configurable TopologicalConfig wrapper (graph_builder2.py:420-483), and node features are rebuilt by staging interface/topology CSVs into temporary folders and calling the legacy node_fea entry point (graph_builder2.py:485-571).

Completed interface/topology/node artefacts are piped into lib/pt_writer.py, which copies them into a staging area and invokes the legacy topoqa/src/graph.create_graph for each decoy (pt_writer.py:116-128). This is the last step before .pt emission and is intentionally identical to the production inference path.
Duplicated Z-Coordinate Bug

The shared create_graph implementation pulls edge histograms from topoqa/src/utils.get_pointcloud_type, where the z-component is mistakenly taken from coord[1] instead of coord[2] (utils.py:85-99). Every atom position becomes [x, y, y], effectively projecting structures into a duplicated-Y plane.

Because Graph Builder 2 relies on that exact function (pt_writer.py:116-128 → graph.create_graph → utils.get_pointcloud_type), the same flattened geometry contaminates every .pt it produces. No logic in graph_builder2.py overrides or corrects the util, so the defect is inherited wholesale.
Impact on Modelling

Edge histograms drive the 11-dimensional edge_attr tensors used throughout training and inference. Collapsing z into y underestimates inter-residue distances whenever the true z-separation is non-zero, skewing histogram bin counts toward shorter ranges.

Training on these distorted graphs teaches the GNN to interpret edge features built from faulty geometry, reducing its ability to generalise and depressing validation metrics (as seen in the elevated MSE during manual evaluation). Inference likewise ranks decoys with the same bias, so fixing training alone cannot help without regenerating inputs.

How to Fix (when approved)

Update topoqa/src/utils.py:87-98 so the third coordinate uses atom.get_coord()[2] (and similarly for the filtered branches); this restores genuine 3D distances for both training and inference pipelines.

Regenerate all .pt graphs via Graph Builder 2 (or the inference pipeline) so the corrected histograms propagate.

Retrain the model with the refreshed data to realign learned weights, then re-export inference checkpoints.


NEW MODEL CODE in /Volumes/PData/Data/Dev/Github/Repos/phd3/qtopo/QTopoQA/qtdaqa/proposed_model_training/ARM_cpu

Training Overhaul

Added a new CPU-first launcher at qtdaqa/proposed_model_training/ARM_cpu/train_topoqa_cpu.py:1-420. It reads YAML config, resolves paths relative to the repo root, and constructs proper datasets that attach dockq targets to every graph (GraphRegressionDataset.__getitem__, lines 147-164) while shuffling the training loader and keeping validation deterministic (lines 248-284). This directly fixes the legacy issues where .y stayed unset, batches weren’t shuffled, and GPU-specific assumptions blocked ARM CPUs.
Introduced early stopping plus a ReduceLROnPlateau scheduler via the CpuTopoQAModule subclass (train_topoqa_cpu.py:174-204), addressing the stagnation risk noted in gat_5_edge1.py. Both callbacks are wired into the Lightning trainer with deterministic settings (train_topoqa_cpu.py:309-343).
Updated the configuration to point at the ARM .pt directory, lowered gradient accumulation to 16 for steadier updates, and exposed tuning knobs for the new callbacks (qtdaqa/proposed_model_training/ARM_cpu/config.yaml:3-23). The run wrapper now simply invokes the new Python entry point and lets extra CLI flags (e.g., --fast-dev-run) flow through (run_training.sh:1-24).
The pipeline logs coverage for train/val splits, warning about the 39/31 missing graphs still present in the current dataset (train_topoqa_cpu.py:238-287), so you can regenerate once the new edge geometry is available.

Differences vs Original GPU Script

Replaces the hard-coded CUDA invocation with accelerator='cpu' while keeping the model architecture intact, ensuring ARM Macs can retrain without edits.
Dataset handling now yields single Data objects rather than [graph_data], eliminating the previous Lightning misalignment and making the training loop easier to follow.
Learning-rate scheduling, early stopping, and configurable batch limits weren’t present before; they should help the model converge to (or below) the 0.0498 MSE target once clean graphs are available.
Testing & Validation

Smoke-tested with ./run_training.sh --fast-dev-run, which exercised the full data-loading path, confirmed per-split coverage, and completed a one-step train/val cycle without GPU usage. No checkpoints are saved in fast-dev mode; the log indicates this and skips final validation accordingly.
Because the currently supplied .pt files still embed the old z-coordinate bug, I recommend waiting for the corrected graphs before launching full training runs; the new code already warns about the missing/misaligned samples so you can re-run once regeneration finishes.
Next Steps

Regenerate the .pt graphs with the fixed edge geometry, then rerun ./run_training.sh (optionally remove runs/ if you want a clean slate) for a full training cycle.
Once a complete graph set exists, monitor the saved CSV logs in qtdaqa/proposed_model_training/ARM_cpu/runs/ to compare validation MSE against the 0.0498 baseline.

Shuffling means the training loader presents samples in a different random order each epoch instead of the fixed order they’re stored in. It’s useful because it breaks correlations between consecutive samples, gives each batch a more representative mix of the data distribution, and helps gradient-based training converge more smoothly.
The newer pipeline enables this for training: build_dataloaders constructs DataLoader(..., shuffle=True, ...) for the train split so batches are reshuffled every epoch, while the validation loader stays deterministic (shuffle=False). See qtdaqa/proposed_model_training/ARM_cpu/model_train_topoqa_cpu.py:214-244.
The legacy GPU script never shuffled—its get_loader helper created both train and validation loaders with shuffle=False, so every epoch saw the graphs in the same order (topoqa_train/train_topoqa.py:95-110).

Give a high level summary of why code base 2 beats code base 1 in a table. What are the lessons to take away from this that are critical to apply for training GNN's or NN's and rank them from highest to lowest in terms of impact as to why code base 2 beats 1. Give an estimate percentage contribution to the overall results for each lesson or reason


Rank	Lever / Lesson	Code Base 1 (topoqa_train)	Code Base 2 (ARM_cpu_only)	Estimated Contribution to MSE Gap
1	Evidence-driven learning-rate scheduling	Fixed LR from optimizer; no scheduler or LR logging.	Adds ReduceLROnPlateau tied to val_loss, plus staged configs that lower LR for fine-tuning.	~45%
2	Early stopping + checkpoint-driven fine-tunes	Runs full 200 epochs regardless of validation; single pass from scratch.	Uses patience-based early stopping and repeatedly resumes the best checkpoint for low-LR refinement (Phase 1/2 scripts).	~30%
3	Data-loader discipline & coverage checks	Deterministic ordering, no shuffle, silent on missing graphs.	Shuffles train batches every epoch, validates graph coverage, seeds workers via Lightning utilities.	~15%
4	Experiment orchestration & logging	One ad-hoc shell command, minimal logging/checkpoint hygiene.	run_training.sh snapshots configs/git state, keeps console logs, manages history, enabling controlled multi-trial sweeps.	~10%
Percentages are heuristic but reflect the observed jump from MSE ≈ 0.14 to ≈ 0.048 when the scheduler/early-stop/fine-tune stack came online.

Takeaways for GNN/NN Training

Prioritise adaptive LR schedules and LR-aware fine-tuning—most of the dramatic improvement came from letting LR respond to validation feedback.
Protect the best checkpoint with early stopping and resume-based fine-tuning rather than running fixed-length epochs.
Keep data loaders stochastic and auditable; shuffled batches and coverage checks guard against hidden bias.
Treat experiment management as infrastructure: reproducible configs, logs, and checkpoints make it possible to iterate until the model converges.

Review the information shared above:
<info>
As long as the .pt files are standard PyG Data objects with an edge_attr tensor, the code will size the GNN to whatever edge feature dimension it finds. Node feature width is inferred the same way.

If those .pt files also carry the metadata dict that the new graph builder writes, the loader captures module IDs, aliases, and variants; without that metadata, it still completes but obviously can’t identify the module name or variant—only the tensor dimensions.

Graphs produced by a different pipeline will work provided they follow the same conventions (edge features in edge_attr, optional node features in x). If they omit either tensor or mix dimensions across files the loader will raise (or at least emit warnings), so consistent structure is still a requirement.
</info>

Explain "As long as the .pt files are standard PyG Data objects with an edge_attr tensor, the code will size the GNN to whatever edge feature dimension it finds. Node feature width is inferred the same way". What does this mean? Can you break down the data structure to clearly explain what this is and why its needed in a particular way? Also why does it seem that the edge feature dimension is critical to the GNN - how come the other features are not having the same importance? Is it because of how the data structure is engineered that this dependency exists or some other reason? Give ways to overcome this issue if its at all possible or desirable


PyG Data fundamentals – A .pt graph produced for TopoQA is a pickled torch_geometric.data.Data object. At minimum it carries:

edge_index ∈ ℕ²×E listing the directed edges.
edge_attr ∈ ℝᴱ×F storing the per-edge feature vector of length F.
x (optional) ∈ ℝᴺ×D containing per-node descriptors; batch, y, and a metadata dict may also be present.
The loader retrieves these tensors, so the graph must follow this layout; otherwise PyG cannot build batches or run GNN layers.
Why the edge dimension drives the GNN – The model uses GNN_edge1_edgepooling, which starts with an MLP nn.Linear(edge_dim, hidden_dim) to encode each edge feature vector before passing it into GATv2ConvWithEdgeEmbedding1. If edge_attr is width F=24, the linear layer must accept 24 inputs; if a different graph arrives with F=11, the layer must be resized. That is why training reads edge_attr.shape[-1] at runtime and rebuilds the network with the matching edge_dim.
Node features matter too, but the default architecture hard-coded 172 components for the “zuhe” mode; we added node_dim as an override so the linear nn.Linear(node_dim, hidden_dim) can adapt when x changes shape. In practice edge features dictate the most fragile front-end because the model assumes edge-aware attention—which breaks immediately if dimensions mismatch—whereas node features fall back to a default width unless overridden.

Is this just a data-structure issue? – Partly. Any neural layer’s input size must match the tensor it receives. Historically we hand-coded those dimensions because the feature set was fixed. Now the dependency is still there, but we remove the manual bookkeeping by interrogating the .pt files. So it’s not that edge features are uniquely important; it’s that the architecture consumes them explicitly via edge_dim, while node features had a hard-coded default until recently.

Mitigation / alternatives

Self-describing graphs – Keep embedding metadata in the .pt files (as the builder now does) so every consumer can configure itself automatically; training already writes the resolved schema to checkpoints for inference reuse.
Canonical featuriser wrappers – Instead of relying on raw Data tensors, wrap the loader so every graph is normalised or padded/truncated to a contract dimension (e.g. always upscale 11-D edges to 24-D with zeros). This adds preprocessing cost but further decouples the network from raw schema changes.
Learned adapters – Replace the first linear layer with a small adaptive module that can accept variable-length inputs (e.g. attention over dict-encoded features, or projecting via MLP that reads a metadata dictionary). This is more complex but might eliminate fixed widths entirely.
Model ensembles – Maintain different checkpoints tuned for specific feature variants (11-D legacy vs 24-D multi-scale) and dispatch at runtime based on detected edge_attr width. The new metadata loader makes detecting the variant trivial; you only need to preserve the appropriate checkpoint.
In short, the code now reads the .pt tensors, matches network dimensions to them, and falls back when metadata is missing. Edge attributes stand out because the first trainable layer is tied directly to their width—so those dimensions must be correct—but the same principle applies to any tensor consumed by the model.

Explain what run_full_pipeline.sh does and what is the high level approach that it takes to training.

Why is a the scan of three seeds 222, 777, 1337 first and why were these chosen ? Are they typically what one uses or arbitrary?

Then explain how fine tuning works and why its required post the first scan of the 3 seeds and cant be done as part of the initial training. How does a resume work from the previously trained model and why is this necessary - is this a typical ML approach


Pipeline Flow

run_full_pipeline.sh first launches a manifest sweep (python -m train_cli batch --manifest manifests/run_all.yaml) to explore every config/seed combo listed in manifests/run_all.yaml and stash fresh runs under training_runs/ (qtdaqa/new_dynamic_features/model_training/run_full_pipeline.sh:26-28, manifests/run_all.yaml:5-23).
After the sweep finishes it scans the new run folders, summarises each via train_cli._summarise_run, and selects the checkpoint with the lowest validation loss (run_full_pipeline.sh:29-79). This isolates the strongest base model before any extra tuning.
Phase 1 fine-tuning resumes that best checkpoint with a dedicated config (configs/sched_boost_finetune.yaml) to keep training but under a tighter schedule/recipe (run_full_pipeline.sh:100-106).
Phase 2 replays the same best checkpoint three more times with seed-specific configs for 101/555/888, giving additional stochastic restarts while preserving the learned representation (run_full_pipeline.sh:108-119). The script ends after these staged runs complete.
Seed Sweep Rationale

The initial manifest hits three baseline seeds (222/777/1337) and repeats them across three schedule variants (default, lower LR, larger batch) to sample performance variance and isolate robust settings (manifests/run_all.yaml:5-23). Those seed values aren’t industry “standards”; they’re simply a spaced trio that’s been used across prior experiments so comparisons stay consistent. Running multiple seeds up front gives a better estimate of how sensitive each configuration is before investing in deeper fine-tuning.
Fine-Tuning Strategy

The fine-tune configs intentionally differ from the sweep configs (usually lower learning rates, longer patience, or extra regularisation). Folding them into the initial sweep would blur the hyperparameter comparison, so the pipeline first finds the strongest baseline and only then applies the specialised schedules.
--resume-from loads the exact weights/checkpoint from the chosen sweep run so Phase 1 (and each Phase 2 seed) continue training from that mature state rather than starting from random initialisation (run_full_pipeline.sh:102-118). This is a common ML pattern: perform a broad search, lock in the best checkpoint, then resume training with a narrower, often more aggressive fine-tuning recipe to squeeze out extra performance without repeating the whole sweep.
Resuming is necessary because fine-tuning assumes the model already captured general structure during the sweep; starting from scratch with the fine-tune configs wouldn’t benefit from the wide exploration that identified a strong baseline, and it would multiply training cost.

The training regime used to be very fast overall but has now slowed down alot for each sweep. Is this because of the model training now being able to dynamically adjust to the structure of the features from the .pt files or the use of spearman values for secondary decisions on learning or both or other reasons? Explain.

Given that the .pt files that constitute the training data features are a total of +- 12GB, does this mean that the train code loads 12GB into memory and then uses it every epoch ? How does this work ?


run_full_pipeline.sh still does three things: sweep every config in manifests/run_all.yaml, pick the lowest validation-loss checkpoint, then resume it for Phase 1 and the three Phase 2 fine-tunes (qtdaqa/new_dynamic_features/model_training/run_full_pipeline.sh:26-119). That flow has not changed, but each stage now processes a much heavier dataset.

The slowdown you’re seeing is dominated by three factors:
• Graph size + batch accumulation – Each .pt graph now carries the 24‑dimensional edge feature stack coming out of the new graph builder. With ~8.7k training graphs (see the coverage logs your recent run printed), Lightning executes 545 forward/backward passes per epoch, and accumulate_grad_batches=16 delays optimizer steps but still runs all 545 batches.
• Single-process data loading – The DataLoader is pinned to num_workers=0 for macOS stability (qtdaqa/new_dynamic_features/model_training/model_train_topoqa_cpu.py:843-859). That means the training loop has to synchronously torch.load each graph from disk, so every batch waits on I/O/CPU deserialization. Raising num_workers or moving to a machine where worker processes behave improves throughput far more than any metric tweak.
• Dynamic schema inference – The dynamic feature detection only samples up to 32 graphs up front to lock in node/edge dimensions (qtdaqa/new_dynamic_features/common/feature_metadata.py:200-360). It adds a short, one-off cost, not per-batch overhead.

Spearman-as-secondary selection runs once per validation epoch and just sorts ~3k numbers; compared to 500+ forward/backward passes, that’s negligible. So the slowdown is primarily compute/data size driven, not the new metadata logic or the extra metric.

You’re not loading 12 GB into RAM at once. Each graph lives in its own .pt file; the GraphRegressionDataset leaves them on disk and the PyG DataLoader pulls a handful of files per batch, collates them, then frees them before the next batch (model_train_topoqa_cpu.py:843-878). The operating system will cache hot pages, but Python never holds the whole corpus in memory. That’s why num_workers and disk speed matter—every epoch still re-reads the graphs, but only the current mini-batch sits in RAM.

Next steps

Try bumping num_workers to 2 or 4 in the sweep configs and monitor stability; the loader will overlap disk reads with compute.
If you have GPU access, switching accelerator to gpu in the configs will dwarf the current CPU runtime, especially with the larger edge feature stacks.
If startup metadata sampling still feels slow, cut max_pt_samples in load_graph_feature_metadata to 8–16; it shaves seconds while keeping the feature dimension checks.
This a mac system but training is using cpu only. If I switch from cpu to mps in the config.yaml will that dramatically improve performance of the training? Is mps supported for the code I'm running? Will the results be consistent or the same as if I ran with cpu just faster? Is the code able to switch over to mps without modification as it stands?


MPS viability

The entry point you’re using (qtdaqa/new_dynamic_features/model_training/model_train_topoqa_cpu.py:10) is deliberately built for CPU. PyTorch Lightning will happily accept accelerator='mps' (model_train_topoqa_cpu.py:281), but the GNN stack behind train_cli relies on torch_geometric scatter/sparse ops that only ship CPU/CUDA kernels today. On MPS these ops either fall back to the CPU or error out, so a pure config flip won’t give you native MPS execution.
Expected performance

Because the heavy PyG layers can’t execute on Apple Silicon cores today, pointing the Trainer at MPS usually just bounces tensors back to CPU, wiping out any speed gain. Even if it ran, a single M2 GPU core isn’t dramatically faster than the CPU for this workload—most wall-clock time is still spent loading thousands of .pt graphs and crunching 500+ batches per epoch on one device (manifests/run_all.yaml:5-23, model_train_topoqa_cpu.py:843-875).
Result consistency

Lightning is already in deterministic mode (deterministic=True, model_train_topoqa_cpu.py:1205), so CPU vs MPS should yield numerically similar results given identical seeds. But because the current build can’t keep the whole graph network on MPS, you’d end up back on CPU anyway, just with extra data copies.
What it would take

To really leverage Apple metal you’d need PyTorch Geometric (and its compiled extensions) with MPS kernels, plus a trainer script that moves data/model to that device. Without those updates—and without reinstalling PyG from a fork that supports MPS—setting accelerator: mps will at best run no faster, and at worst crash once the first scatter/sparse op fires.
Natural next step: keep the configs on CPU, and reclaim time by raising num_workers (if macOS allows) or moving runs to a CUDA-capable box.

If I move to an x86 box with more cores and with an nvidia gpu, should I do to get good results and improved speed? Mention was made of "most wall-clock time is still spent loading thousands of .pt graphs and crunching 500+ batches per epoch on one device" - what does imply and how can I overcome this on the x86 box?

What can I do on apple for now to speed up training apart from mps which will not work?


On an x86 + NVIDIA box

Move training to the CUDA trainer entry point (accelerator: gpu, devices: 1 or more). PyTorch Geometric has full CUDA kernels, so your GNN layers and attention blocks will finally run on GPU instead of CPU-only. You’ll see the biggest gains by:
installing the matching CUDA wheels for PyTorch, PyTorch Geometric, torch-scatter/sparse (the “extra-index” commands from the PyG install docs),
bumping num_workers in every config so the DataLoader can stream .pt graphs from disk while the GPU trains (start with 4–8 workers and scale until the GPU stays busy),
raising batch_size or reducing accumulate_grad_batches once the model fits in GPU memory; this cuts iteration count per epoch.
The “500+ batches per epoch” comment means each epoch currently walks through ~8.7k graphs in batches of 16 on a single device, with 16 gradient accumulations to mimic batch 256. On a multi-core CPU + fast SSD, using more workers plus a GPU means those 500 loader steps can overlap disk I/O and compute instead of running serially. With multiple GPUs you can also set devices: N and strategy: ddp to split the batch across cards.
Monitor CPU, disk, and GPU usage (nvidia-smi, iostat). If the GPU sits idle, increase workers; if disk is saturated, store the dataset on NVMe or a local SSD instead of network storage.
Apple Silicon options today

Stay on CPU but raise num_workers above 0; macOS spawn penalties exist, yet on M-series chips 2 workers often work fine. Watch for hangs; revert to 0 if they appear.
Use mixed precision (precision: 16-mixed) to lighten math per batch; even on CPU this trims some work, though gains are modest. It becomes more relevant once you have GPU access.
Reduce accumulate_grad_batches (e.g., 8 instead of 16) and compensate by shrinking batch_size slightly, which halves optimizer updates per epoch.
Cache hot .pt graphs on an external SSD or RAM disk to improve read bandwidth.
Run targeted sweeps (fewer configs/seeds) before the full manifest so you’re not spending hours on runs that you’ll discard.
When you move to the NVIDIA rig, replicate the environment (requirements, configs) but flip the accelerator, bump loader workers and batch sizing, and make sure the dataset lives on fast local storage. That combination tackles both bottlenecks: disk serialization and single-device math.
