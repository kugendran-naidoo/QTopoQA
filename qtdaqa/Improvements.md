Improvements

- modify calculate_interface.py
    - remove redundant PDB parser
    - remove all hard-coded values
    - add KDTree implementation to improve parsing PDB file
        - https://chatgpt.com/g/g-p-68c3bcec209081919551218d8aba08b5-shell-python/c/68de63af-7730-8326-9acb-e43c45eeac51

- modify topological_features.py
    - remove all hard-coded values


Issues

Pipeline Weaknesses

Interface extraction only attempts model.pdb files (topoqa/src/get_interface.py:61-65); decoys distributed as .cif/.mmcif will never enter the pipeline unless duplicate .pdb copies exist, leading to missing graphs and lower recall.

Node features depend on mkdssp being discoverable (topoqa/src/node_fea_df.py:43-73); if the binary is absent the script raises, yet the shell wrappers have no pre-flight check for it, so production runs can fail late.

utils.get_pointcloud_type writes the z-coordinate as coord[1] (topoqa/src/utils.py:85-99), flattening 3D distances into a plane and corrupting all histogram-based edge attributes; this likely inflates inference variance and should be corrected before retraining.

Graph construction again hard-codes .pdb input (topoqa/src/graph.py:12-28) and never attaches labels, so the .pt files generated by inference lack targets; training scripts must rewrite .y from CSVs every time.

The per-target embedding writer assumes decoy IDs differ only by simple suffixes and normalises with regexes (topoqa/k_mac_inference_pca_tsne4.py:175-276); collisions between cleaned IDs can silently mix labels.

Training Pipeline

topoqa_train/gpu_run_training.zsh:1-12 always points at topoqa_train/graph_data, so the ARM-generated .pt files are ignored unless you swap directories manually; it also unconditionally requests a CUDA device.

topoqa_train/train_topoqa.py:77-99 returns [graph_data] from the dataset, but the collate function expects Data objects and Lightning later indexes train_batch[0], so the first tensor column is treated as a sample; combined with the missing .y assignment (train_topoqa.py:84-87), the GPU path cannot learn meaningful weights.

Training and validation loaders never shuffle (train_topoqa.py:98-115), so even if the script were fixed the optimiser would see data in a fixed order with an effective batch of 16×32=512, giving only ~17 optimiser steps per epoch.

The working training entry point is topoqa_train/k_mac_train_topoqa.py, which rebuilds labels, sanitises stale batch attributes, and runs on CPU (topoqa_train/k_mac_train_topoqa.py:134-248); it also copies the best checkpoint for reproducibility.

gat_5_edge1.py:205-282 logs only MSE yet keeps per-target Pearson/Spearman metrics; without explicit early stopping or LR scheduling, the high accumulate setting can still plateau, so monitoring val_loss is essential.

Mini Tests

Coverage against the ARM .pt directory is high but not complete: 8684/8723 training labels and 3366/3397 validation labels had matching graphs (Python check run in-session).

Using manual batching (Batch.from_data_list) with the published topoqa/model/topoqa.ckpt, I measured train MSE 0.0133 and val MSE 0.0557 on the ARM graphs; MAEs were 0.0747 and 0.1483 respectively. This is close but ~0.006 above the reported 0.0498 target, suggesting coverage gaps or subtle feature drift.

39 training and 31 validation decoys lacked graphs; they skew the dockq distribution slightly towards lower-quality models in the ARM set, so supplementing or regenerating those graphs should help close the remaining MSE gap.

CPU vs GPU

The maintained trainer already targets CPU (topoqa_train/k_mac_train_topoqa.py:237-243) and delivers a Lightning best_model_score of 0.0498, so matching the reference MSE is feasible without a GPU provided you accept longer epochs (~5-6× slower in earlier runs).

GPU acceleration benefits come mainly from joblib feature extraction and Lightning optimisation time; inference itself is CPU-bound until graph tensors are loaded, so a GPU mostly shortens training iterations rather than improving fit quality.

If you remain on CPU, keep accumulate_grad_batches modest and prefer deterministic seeds (k_mac_train_topoqa.py:126-154) to stabilise convergence; the metric delta we observed appears attributable to missing graphs rather than device choice.

Next Steps

Repair feature geometry (utils.py:85-99) and regenerate graphs so all CSV labels have matching .pt artefacts before retraining.

Either patch train_topoqa.py to set graph_data.y, return plain Data objects, and use shuffling, or retire it in favour of k_mac_train_topoqa.py; then rerun training on the updated graph set and re-evaluate MSE on the ARM data.

Graph Builder 2 Overview

qtdaqa/graph_builder2/graph_builder2.py reproduces the full inference data flow locally: it validates dataset/work/log paths, extracts interfaces for every .pdb/.cif via new_calculate_interface.process_pdb_file, rounds coordinates, and records per-model logs (graph_builder2.py:260-418).

Topological descriptors are recomputed with the configurable TopologicalConfig wrapper (graph_builder2.py:420-483), and node features are rebuilt by staging interface/topology CSVs into temporary folders and calling the legacy node_fea entry point (graph_builder2.py:485-571).

Completed interface/topology/node artefacts are piped into lib/pt_writer.py, which copies them into a staging area and invokes the legacy topoqa/src/graph.create_graph for each decoy (pt_writer.py:116-128). This is the last step before .pt emission and is intentionally identical to the production inference path.
Duplicated Z-Coordinate Bug

The shared create_graph implementation pulls edge histograms from topoqa/src/utils.get_pointcloud_type, where the z-component is mistakenly taken from coord[1] instead of coord[2] (utils.py:85-99). Every atom position becomes [x, y, y], effectively projecting structures into a duplicated-Y plane.

Because Graph Builder 2 relies on that exact function (pt_writer.py:116-128 → graph.create_graph → utils.get_pointcloud_type), the same flattened geometry contaminates every .pt it produces. No logic in graph_builder2.py overrides or corrects the util, so the defect is inherited wholesale.
Impact on Modelling

Edge histograms drive the 11-dimensional edge_attr tensors used throughout training and inference. Collapsing z into y underestimates inter-residue distances whenever the true z-separation is non-zero, skewing histogram bin counts toward shorter ranges.

Training on these distorted graphs teaches the GNN to interpret edge features built from faulty geometry, reducing its ability to generalise and depressing validation metrics (as seen in the elevated MSE during manual evaluation). Inference likewise ranks decoys with the same bias, so fixing training alone cannot help without regenerating inputs.

How to Fix (when approved)

Update topoqa/src/utils.py:87-98 so the third coordinate uses atom.get_coord()[2] (and similarly for the filtered branches); this restores genuine 3D distances for both training and inference pipelines.

Regenerate all .pt graphs via Graph Builder 2 (or the inference pipeline) so the corrected histograms propagate.

Retrain the model with the refreshed data to realign learned weights, then re-export inference checkpoints.


NEW MODEL CODE in /Volumes/PData/Data/Dev/Github/Repos/phd3/qtopo/QTopoQA/qtdaqa/proposed_model_training/ARM_cpu

Training Overhaul

Added a new CPU-first launcher at qtdaqa/proposed_model_training/ARM_cpu/train_topoqa_cpu.py:1-420. It reads YAML config, resolves paths relative to the repo root, and constructs proper datasets that attach dockq targets to every graph (GraphRegressionDataset.__getitem__, lines 147-164) while shuffling the training loader and keeping validation deterministic (lines 248-284). This directly fixes the legacy issues where .y stayed unset, batches weren’t shuffled, and GPU-specific assumptions blocked ARM CPUs.
Introduced early stopping plus a ReduceLROnPlateau scheduler via the CpuTopoQAModule subclass (train_topoqa_cpu.py:174-204), addressing the stagnation risk noted in gat_5_edge1.py. Both callbacks are wired into the Lightning trainer with deterministic settings (train_topoqa_cpu.py:309-343).
Updated the configuration to point at the ARM .pt directory, lowered gradient accumulation to 16 for steadier updates, and exposed tuning knobs for the new callbacks (qtdaqa/proposed_model_training/ARM_cpu/config.yaml:3-23). The run wrapper now simply invokes the new Python entry point and lets extra CLI flags (e.g., --fast-dev-run) flow through (run_training.sh:1-24).
The pipeline logs coverage for train/val splits, warning about the 39/31 missing graphs still present in the current dataset (train_topoqa_cpu.py:238-287), so you can regenerate once the new edge geometry is available.

Differences vs Original GPU Script

Replaces the hard-coded CUDA invocation with accelerator='cpu' while keeping the model architecture intact, ensuring ARM Macs can retrain without edits.
Dataset handling now yields single Data objects rather than [graph_data], eliminating the previous Lightning misalignment and making the training loop easier to follow.
Learning-rate scheduling, early stopping, and configurable batch limits weren’t present before; they should help the model converge to (or below) the 0.0498 MSE target once clean graphs are available.
Testing & Validation

Smoke-tested with ./run_training.sh --fast-dev-run, which exercised the full data-loading path, confirmed per-split coverage, and completed a one-step train/val cycle without GPU usage. No checkpoints are saved in fast-dev mode; the log indicates this and skips final validation accordingly.
Because the currently supplied .pt files still embed the old z-coordinate bug, I recommend waiting for the corrected graphs before launching full training runs; the new code already warns about the missing/misaligned samples so you can re-run once regeneration finishes.
Next Steps

Regenerate the .pt graphs with the fixed edge geometry, then rerun ./run_training.sh (optionally remove runs/ if you want a clean slate) for a full training cycle.
Once a complete graph set exists, monitor the saved CSV logs in qtdaqa/proposed_model_training/ARM_cpu/runs/ to compare validation MSE against the 0.0498 baseline.

Shuffling means the training loader presents samples in a different random order each epoch instead of the fixed order they’re stored in. It’s useful because it breaks correlations between consecutive samples, gives each batch a more representative mix of the data distribution, and helps gradient-based training converge more smoothly.
The newer pipeline enables this for training: build_dataloaders constructs DataLoader(..., shuffle=True, ...) for the train split so batches are reshuffled every epoch, while the validation loader stays deterministic (shuffle=False). See qtdaqa/proposed_model_training/ARM_cpu/model_train_topoqa_cpu.py:214-244.
The legacy GPU script never shuffled—its get_loader helper created both train and validation loaders with shuffle=False, so every epoch saw the graphs in the same order (topoqa_train/train_topoqa.py:95-110).